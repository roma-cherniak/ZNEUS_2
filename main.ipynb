{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### create path to my data on pc",
   "id": "eae467823ab77c72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_path = r\"C:\\Users\\1\\Desktop\\University_semeter_3\\ZNEUS\\archive\\ASL_Dataset\\Train\"",
   "id": "938b9b99d28b7b45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### create csv file with image pases and labels",
   "id": "f72c652e532e6d4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = []\n",
    "\n",
    "for folder in os.listdir(train_path):\n",
    "    folder_path = os.path.join(train_path, folder)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        for image in os.listdir(folder_path):\n",
    "            if image.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_path = os.path.join(folder_path, image)\n",
    "                data.append({'path': image_path, 'label': folder})\n",
    "\n",
    "# Створюємо DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Зберігаємо в CSV\n",
    "df.to_csv('dataset.csv', index=False)\n",
    "print(f\" {len(df)} rows\")\n",
    "print(f\"Clases: {df['label'].unique()}\")\n"
   ],
   "id": "80d94f080daf2a28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds = pd.read_csv('dataset.csv')\n",
    "ds.head()\n"
   ],
   "id": "8b7d268b7ed46797",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds.info()",
   "id": "c6f58115a97c275f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds.duplicated().sum()",
   "id": "7c570ca5d4c5e29e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds = ds.sample(frac=1, random_state=42).reset_index(drop=True)",
   "id": "bdcb59ca6b30d7c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds_answer =ds.iloc[:,-1:]\n",
    "ds_answer.head()"
   ],
   "id": "f9ca615ba1790b41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds_train = ds.iloc[:,:-1]\n",
    "ds_train.head()"
   ],
   "id": "d78201a2717ebbec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(3):\n",
    "    img = Image.open(ds_train.iloc[i,0])\n",
    "    img.show()\n",
    "\n",
    "\n"
   ],
   "id": "5a2227b84494a9f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "img_path = df.iloc[0]['path']\n",
    "img = Image.open(img_path)\n",
    "\n",
    "img_tensor = transform(img)\n",
    "\n",
    "print(img_tensor.shape)"
   ],
   "id": "656b0aa30a4cd9c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# make dataset with tensors\n",
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        unique_labels = sorted(self.data['label'].unique())\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data.iloc[idx]['path'])\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        label_idx = self.label_to_idx[label]\n",
    "        return self.transform(img), label_idx\n",
    "\n",
    "\n",
    "dataset = ASLDataset('dataset.csv', transform=transform)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True)"
   ],
   "id": "7163d2097f649d1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:02.135084Z",
     "start_time": "2025-11-17T22:28:02.123656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv1 = nn.Conv2d(1,6,3,1)\n",
    "conv2 = nn.Conv2d(6,12,3,1)\n",
    "conv3 = nn.Conv2d(12,18,3,1)\n",
    "conv4 = nn.Conv2d(18,24,3,1)\n",
    "conv5 = nn.Conv2d(24,32,3,1)"
   ],
   "id": "725aaa1ecba31720",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:11.894567Z",
     "start_time": "2025-11-17T22:28:11.883343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.relu(conv1(img_tensor.unsqueeze(0)))\n",
    "x.shape"
   ],
   "id": "1d0afe8d39feb102",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 398, 398])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:13.114095Z",
     "start_time": "2025-11-17T22:28:13.106130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.max_pool2d(x,2,2)\n",
    "x.shape"
   ],
   "id": "30339b748ef0514a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 199, 199])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:14.456667Z",
     "start_time": "2025-11-17T22:28:14.448998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.relu(conv2(x))\n",
    "x.shape"
   ],
   "id": "5009a24ce4316c24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 197, 197])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:15.852994Z",
     "start_time": "2025-11-17T22:28:15.844832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.max_pool2d(x,2,2)\n",
    "x.shape"
   ],
   "id": "9c6d86e493248289",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 98, 98])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:17.132580Z",
     "start_time": "2025-11-17T22:28:17.125770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.relu(conv3(x))\n",
    "x.shape"
   ],
   "id": "7f4ce40bd61b3407",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 96, 96])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:18.303376Z",
     "start_time": "2025-11-17T22:28:18.298681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.max_pool2d(x,2,2)\n",
    "x.shape"
   ],
   "id": "ce5037716c81aaea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 48, 48])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:19.426223Z",
     "start_time": "2025-11-17T22:28:19.421393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.relu(conv4(x))\n",
    "x.shape"
   ],
   "id": "e79402dd605c0b7d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 46, 46])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:20.661167Z",
     "start_time": "2025-11-17T22:28:20.654530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.max_pool2d(x,2,2)\n",
    "x.shape"
   ],
   "id": "3b59a3c0f1b6cd95",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 23, 23])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:21.820473Z",
     "start_time": "2025-11-17T22:28:21.812467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.relu(conv5(x))\n",
    "x.shape"
   ],
   "id": "f53e0470fb8ab9e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 21, 21])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:28:23.534379Z",
     "start_time": "2025-11-17T22:28:23.530373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.max_pool2d(x,2,2)\n",
    "x.shape"
   ],
   "id": "cadd38a1950f81f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 10, 10])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:29:07.108555Z",
     "start_time": "2025-11-17T22:29:07.100266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,3,1)\n",
    "        self.conv2 = nn.Conv2d(6,12,3,1)\n",
    "        self.conv3 = nn.Conv2d(12,18,3,1)\n",
    "        self.conv4 = nn.Conv2d(18,24,3,1)\n",
    "        self.conv5 = nn.Conv2d(24,32,3,1)\n",
    "        self.fc1 = nn.Linear(32*10*10,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,29)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x= F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x= F.relu(self.conv5(x))\n",
    "        x= F.max_pool2d(x,2,2)\n",
    "        x = x.view(-1,32*10*10)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x= self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "id": "dfe14b37968d605a",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:29:08.854145Z",
     "start_time": "2025-11-17T22:29:08.847394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ConvolutionalNetwork()\n",
    "model\n",
    "\n"
   ],
   "id": "852d3a9870e49457",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNetwork(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(12, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(18, 24, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv5): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3200, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=29, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:43:06.711503200Z",
     "start_time": "2025-11-17T22:29:41.984459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "Number_of_epochs = 2\n",
    "for epoch in range(Number_of_epochs):\n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        # Перевір shape (тільки раз)\n",
    "        if i == 0 and epoch == 0:\n",
    "            print(f\"Images: {images.shape}, Labels: {labels.shape}\")\n",
    "\n",
    "        y_pred = model(images)\n",
    "        loss = loss_func(y_pred, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/10], Step [{i+1}], Loss: {loss.item():.4f}')"
   ],
   "id": "b19954d19dd7607b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([32, 1, 400, 400]), Labels: torch.Size([32])\n",
      "Epoch [1/10], Step [1], Loss: 3.3541\n",
      "Epoch [1/10], Step [101], Loss: 3.3371\n",
      "Epoch [1/10], Step [201], Loss: 2.9925\n",
      "Epoch [1/10], Step [301], Loss: 1.6433\n",
      "Epoch [1/10], Step [401], Loss: 0.7734\n",
      "Epoch [1/10], Step [501], Loss: 0.4346\n",
      "Epoch [1/10], Step [601], Loss: 0.1265\n",
      "Epoch [1/10], Step [701], Loss: 0.4347\n",
      "Epoch [1/10], Step [801], Loss: 0.3198\n",
      "Epoch [1/10], Step [901], Loss: 0.4645\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
